{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So first things first. Make sure you complete the routine steps:\n",
    "1. Enter your netID into the github repo and then clone the repo to your local computer\n",
    "2. Create a new conda environment using: \"conda create -n hw7 python\"\n",
    "3. Activate the new environment: \"conda activate hw7\"\n",
    "4. There are 4 packages you will be needing for this homework: gym, matplotlib, numpy and pytest. \n",
    "   Install these packages using \"pip install -r requirements.txt\"\n",
    "5. Now you are all set to start."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-armed Bandit\n",
    "\n",
    "So lets implement the simplest reinforcement learning (RL) recipe: **Multi-Armed Bandit (MAB)**.\n",
    "For getting a concrete idea about the problem begin by having a quick look at Chapter 2 of Sutton and Barto's book on RL: http://incompleteideas.net/book/RLbook2018.pdf. In summary, the two core concepts that this algorithm drives through are:\n",
    "\n",
    "1. Given a hypothetical setting where you are repeatedly faced with making a choice from **k** different options (each with\n",
    "different reward amounts), how do you choose the *one* sweet option everytime so that you maximize your reward over a period of time? \n",
    "2. Along the course of your learning the best possible action, how do you balance between **exploiting** what you have       learned so far (i.e. choosing the best action from your current reward value estimation of the k actions) and **exploring** what you may not have learned yet (with the gamble that it may improve your current estimates of rewards and 'open up new avenues')? Sounds philosophical eh?\n",
    "\n",
    "Here, we will be using the *gym* library from OpenAI that provides some really nice ready-made and fully customizable learning environments for testing your own RL recipe (https://gym.openai.com/docs/). Lets begin by importing *gym*:\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>ATTENTION:</b> You might need to: pip install gym\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Either you can create your own custom environment (the one for testing MAB i.e. Slot Machines is already done for you in the script slot_machines.py). or load a pre-existing one. So let us familiarize with some of the common aspects of a simple *gym* environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of possible actions in Frozen Lake = 4\n",
      "Number of possible states in Frozen Lake = 16\n",
      "Beginning in state = 0\n",
      "Taking action number 3...\n",
      "New state is = 0, reward value = 0.0\n",
      "Time to reset? False\n",
      "Taking action number 3...\n",
      "New state is = 0, reward value = 0.0\n",
      "Time to reset? False\n",
      "Taking action number 3...\n",
      "New state is = 1, reward value = 0.0\n",
      "Time to reset? False\n",
      "Taking action number 1...\n",
      "New state is = 0, reward value = 0.0\n",
      "Time to reset? False\n",
      "Taking action number 2...\n",
      "New state is = 0, reward value = 0.0\n",
      "Time to reset? False\n",
      "Taking action number 3...\n",
      "New state is = 0, reward value = 0.0\n",
      "Time to reset? False\n",
      "Taking action number 0...\n",
      "New state is = 4, reward value = 0.0\n",
      "Time to reset? False\n",
      "Taking action number 2...\n",
      "New state is = 8, reward value = 0.0\n",
      "Time to reset? False\n",
      "Taking action number 3...\n",
      "New state is = 4, reward value = 0.0\n",
      "Time to reset? False\n",
      "Taking action number 1...\n",
      "New state is = 4, reward value = 0.0\n",
      "Time to reset? False\n",
      "Taking action number 1...\n",
      "New state is = 5, reward value = 0.0\n",
      "Time to reset? True\n"
     ]
    }
   ],
   "source": [
    "# Frozen Lake is a 4 x 4 grid world where your bot navigates from start to goal\n",
    "# by treading on thin ice. Every step that it takes is ridden with uncertainty:\n",
    "# The ice in the next step could be hard enough to withstand the weight of your\n",
    "# bot or it could crack under its weight! The objective of this gruesome game is\n",
    "# to reach the goal position in minimum steps and of course avoid meeting a chilly\n",
    "# fate in the dark, deep and cold water underneath.\n",
    "# To learn more: https://reinforcement-learning4.fun/2019/06/16/gym-tutorial-frozen-lake/\n",
    "import numpy as np\n",
    "myenv = gym.make('FrozenLake-v0') \n",
    "myenv.seed(0) # for repeatability\n",
    "\n",
    "# Every environment has an action space i.e. the actions you can take:\n",
    "print(\"Number of possible actions in Frozen Lake = \" + str(myenv.action_space.n))\n",
    "\n",
    "# and a state space i.e. the different states in the environment:\n",
    "print(\"Number of possible states in Frozen Lake = \" + str(myenv.observation_space.n))\n",
    "\n",
    "# You begin by resetting your environment to the intial state:\n",
    "intial_state = myenv.reset()\n",
    "print(\"Beginning in state = \" + str(intial_state))\n",
    "\n",
    "# Now you (actually your AI bot) wanders through the evironment...step by step\n",
    "# Its time to reset the game when your steps onto thin ice and it cracks under\n",
    "# its weight!\n",
    "done = False\n",
    "while done is False:\n",
    "    # first choose your action:\n",
    "    action_idx = np.random.randint(myenv.action_space.n)\n",
    "\n",
    "    # Now perform this action:\n",
    "    print(\"Taking action number \" + str(action_idx) + \"...\")\n",
    "    observation, reward, done, info = myenv.step(action_idx) \n",
    "    print(\"New state is = \" + str(observation) + \", reward value = \" + str(reward) + \"\\nTime to reset? \" + str(done))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refer to the OpenAI website for details of the values returned by *myenv.step(action_index)*. Here is a brief summary: \n",
    "\n",
    "> 1. In this homework we will not need *info*.\n",
    "> 2. *observation* is an environment-specific object representing your observation of the environment. For the purposes of this homework its the the new state value of the environement as a result of your action.\n",
    "> 3. *reward* is...reward.\n",
    "> 4. *done* states whether itâ€™s time to reset the environment again. When *done* is true, its time to reset (because its game over! Or else your bot gets stuck in the deep, dark and cold waters...forever).\n",
    "\n",
    "So to reset your environment (which pulls your AI back to the start of the game) you simply:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Returned to intial state = 0\n"
     ]
    }
   ],
   "source": [
    "starting_state = myenv.reset()\n",
    "print(\"Returned to intial state = \" + str (starting_state))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Slot Machines environment is already written for you. For using this environment in your own testing script (and also for running the algorithm for your free responses) all you need to do is follow these steps carefully:\n",
    "\n",
    "> 1. Copy the slot_machines.py script to your *gym\\envs\\classic_control* folder (For Anaconda users it should be somewhere like: *Anaconda3\\Lib\\site-packages\\gym\\envs\\classic_control*).\n",
    "> 2. Edit the __init__.py file in *gym\\envs\\classic_control* to auto import this environment when you import gym:\n",
    "     **from gym_slot.envs.slot_machines import SlotMachines**\n",
    "> 3. Register your newly added environment by editing the __init__.py file in *gym\\envs* (scroll down to the 'Classic' section and add the following lines at the end of this section):\n",
    "**register(\n",
    "    id='SlotMachines-v0',\n",
    "    entry_point='gym.envs.classic_control:SlotMachines',\n",
    ")**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>ATTENTION:</b> Make sure to complete the above steps before running the next snippet. You might need to reload this notebook at this point to ensure the modifications to your gym module takes effect properly.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us try loading the environment so as to ensure that everything so far is working as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myenv = gym.make('SlotMachines-v0')\n",
    "myenv.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There should not be any error at this point if you have done everything correctly. Feel free to reach out through **CampusWire** in case you face any issues.\n",
    "\n",
    "Now that we have everything setup, let us walk through the *fit* function of MAB. The key aspect of the MAB problem is that there is only one state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of possible states in Slot Machines = 1\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of possible states in Slot Machines = \" + str(myenv.observation_space.n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So all that you really care about is how to estimate the reward values for each possible action over a certain time (i.e. steps)\n",
    "As per the pseudocode in page 32 of Sutton and Barto's book, begin by setting up two variables for your *fit* function:\n",
    "\n",
    "Q = vector to store the current reward estimate for each possible action (all zeros at the beginning).\n",
    "\n",
    "N = vector to store the number of steps taken with each possible action (all zeros at the beginning).\n",
    "\n",
    "And where do you get the number of possible actions from? Scroll up to find the answer if you don't recall. \n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>ATTENTION:</b> Always make it a habit to reset your environment at the beginning of all functions.\n",
    "</div>\n",
    "\n",
    "So your *fit* function has acess to two parameters:\n",
    "1. steps / iterations\n",
    "2. epsilon\n",
    "\n",
    "Remember the two core concepts we talked about at the beginning of this notebook? Now it all makes sense right! Yup you got it:\n",
    "1. Sutton's book says you iterate forever till you get your best estimate. Well, if you iterate forever, you will actually land up getting the actual reward values rather than their estimates. That's an irony! So lets be practical and iterate through a finite number of steps so that we can finish the homework before the deadline.\n",
    "\n",
    "    So in each step of your *for* loop you do the following:\n",
    "        a. Be greedy: choose the action for which you get the best estimated reward (Hint: use the vector Q to do this).\n",
    "        b. Do you have a tie for the best action in the step a? Then be fair and break the tie randomly.\n",
    "        c. Take the step in the environment and collect your reward. (You know how to do this right? Hint: Scroll up)\n",
    "        d. Update the step count for the action you just took (Hint: use the vector N to do this).\n",
    "        e. Now the crux of the algorithm, the one key line that defines it all...update your reward estimate as:\n",
    "           Q[action_you_took] = Q[action_you_took] + (1/N[action_you_took]) * (reward - Q[action_you_took])\n",
    "        f. As always don't forget to check the status of DONE! Or else your bot is doomed for eternity...\n",
    "Your homework code requires you to do one additional thing. You need to record the mean reward every **(s = floor(steps / 100))** step. There are number of ways you could do that easily. One simple way is to keep a separate varaible **reward_record** to sum succesive rewards you get each step till you encounter **mod(i/s) == 0** where **i** is your *for* loop counter. At this point you store the amount of summed up reward so far (after dividing it by **s**) and reset **reward_record** to zero.\n",
    "\n",
    "\n",
    "2. Wait. Where does **epsilon** come into all this? Your hunch was right...the other core concept of this algorithm: *To explore or to exploit...thats the question.* How do you decide? You don't. You just let *the slings and arrows of outrageous fortune* do it for you. Humor apart, **epsilon** (value ranging from 0-1) is the probability that your bot will not be greedy. So in the step **a** of the pseudocode above, instead of choosing the action with the current maximum estimated reward everytime, you choose an action randomly from all available actions with **epsilon** probability (yes by doing so you also give the action with maximum reward a fair chance of getting chosen after all...so democratic!). There are a number of ways you could do it, my personal favorite is: \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augur advices to be greedy and take the known path.\n"
     ]
    }
   ],
   "source": [
    "epsilon = 0.2\n",
    "augur_says = np.random.choice(['to exploit','to explore'], 1, p=[1.0 - epsilon, epsilon])[0]\n",
    "if augur_says == 'to exploit':\n",
    "    print(\"Augur advices to be greedy and take the known path.\")\n",
    "else:\n",
    "    print(\"Augur advices that the gods are with you in exploring the unknown.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thats it! Your first RL recipe is almost ready. All that is left is to do the magic i.e. predict. If you have been able to weather the storm so far, predicting should be a breeze. Essentially all your bot needs to do is follow the best possible course as set by your fitted Q vector (state_action_values). Remember that for MAB, there is only one state:\n",
    "1. Press reset.\n",
    "2. Set **done** to False.\n",
    "3. Initialize **states**, **actions** and **rewards** as empty lists (you fill them up as you sail through the environment).\n",
    "4. While **done** is False:\n",
    "   \n",
    "       a. Select the action with the maximum reward.\n",
    "       b. Perform that action\n",
    "       c. Collect your reward.\n",
    "\n",
    "Remember to apppend the **state**, **action** and **reward** values for each iteration above to the corresponding lists.\n",
    "Thats it! Return the **states**, **actions** and **rewards** lists (after converting them to arrays) and you're done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-learning\n",
    "\n",
    "As earlier, begin by giving Sutton and Barto's book on temporal difference learning a quick read (Chapter 6). Temporal difference is a class of a number of algorithms of which Q-learning is a member. Why they are called 'temporal difference' learners is based on the reward updation step as you shall see soon (step 5 of *fit* function below). For the purposes of completing the code, here is what you need to know:\n",
    "1. Now you can no longer assume that there is only one state in your environment. Refer to the Frozen Lake problem above for an example. Every action takes you to a new state where taking the same action as in the previous state does not necessarily elicit the same reward. So instead of considering actions alone, you need to consider (state,action) pairs. Thats the key concept.\n",
    "2. There is a difference in the reward learning step...we will deal with that in a while.\n",
    "\n",
    "So lets dive in:\n",
    "This time your *fit* function has access to 2 additional parameters, **discount** and **adaptive**. You will soon see how we will use these in our code...\n",
    "as in MAB, begin by setting up the following two variables:\n",
    "\n",
    "Q = a matrix to store the current reward estimate for each possible state-action pair (all zeros at the beginning).\n",
    "\n",
    "N = a matrix to store the number of steps taken for each possible state-action pair (all zeros at the beginning).\n",
    "\n",
    "So if A is the number of possible actions, and S is the number of possible states in the environment, each of the above two matrcies should be of size (S,A). By now you surely know how to get S and A right?\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>ATTENTION:</b> As always, rememeber to press the RESET button at the beginning!\n",
    "</div>\n",
    "\n",
    "Then, for the *for* loop its pretty much the same game as in MAB (except for a few subtle differences as you will see):\n",
    "1. As in MAB be (1 - epsilon)% greedy and epsilon% non-greedy in choosing your action. Only this time be wary of the state you are currently in i.e. best action is the action with the maximum estimated reward for the current state. (Hint: use the matrix Q). \n",
    "2. Do you have a tie for the best action in step 1? Then be fair and break the tie randomly.\n",
    "3. Take the step in the environment and collect your reward. Also record the **state_after_action** your bot enters as a result of taking the action. (You know how to do this right? Hint: Scroll up)\n",
    "4. Update your step count for the action you just took. Again, be wary of the state you are in. (Hint: use the matrix N)\n",
    "5. Now the crux of the algorithm, the subtle difference from MAB...update your reward estimate as:\n",
    "*Q[state_before_action, current_action] = Q[state_before_action, current_action] + (1.0 / N[state_before_action, current_action]) x (reward + **discount** x max(Q[state_after_action,:]) - Q[state_before_action, current_action])*. Very carefully note how we use the the *state before the current action* i.e. the **state from the previous iteration** and the *state after taking the current action* i.e. the **state you enter in the current iteration after performing the action**. Note how the reward updation is based on an error value measuring the difference in the estimate for the current step as compared to the previous step. Hence the name *temporal difference*.\n",
    "6. Update the **state_before_action** using: **state_before_action = state_after_action**.\n",
    "6. As always don't ignore DONE! Or else...\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>SUBTLETY ALERT:</b> What state is your bot at the beginning of the game? Hint: Did you press the RESET button at the beginning?\n",
    "</div>\n",
    "\n",
    "Ah yes, your homework requires you to record the mean reward every **(s = floor(steps / 100))** step. Follow the same lead as in MAB...\n",
    "\n",
    "Finally, lets deal with **adaptive**. So you can rely on the *augur* (read *np.random.choice*) to decide when to **explore**  and when to **exploit**. But hey! Thats what the medieval people did right? So as society evolved it became self-reliant and learned to trust its own experiences in deciding things rather than resort to fantasy. Likewise, as your bot gains experience in the environment with increasing number of steps it takes, the estimates for the rewards for (state,action) pairs get better and better. So the strategy is to gradually shift from epsilon% exploration (and consequently [1 - epsilon]% exploitation) to 100% exploitation since you know your estimates are as good as it can get in finite time. One systematic way to do this is to introduce a hyper-parameter **progress** which is simply how many steps your bot has taken so far (as a fraction of the maximum number of steps it is supposed to take). For your convenience, your script has been set-up so that **progress** based **epsilon** is obtained simply by calling the function *_get_epsilon(**progress**)*. So in step 1 of the the Q-learning *fit* function, for every iteration, you always use the **epsilon** valued obtained from *_get_epsilon(**progress**)* where **progress** = i / steps (assuming *i* is your *for* loop counter).\n",
    "\n",
    "Now at this stage, *predict* is an absolute no-brainer for you. As you guessed its the same as in MAB. \n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>SUBTLETY ALERT:</b> When predicting in Q-learning, remember that the action that yields the maximum reward depends on the current state. Hint: use the fitted Q matrix for this.\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
